{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e032351a-cd85-4419-8b36-67ce2170895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Activation Function\n",
    "\n",
    "# An activation function in artificial neural networks is a mathematical function that introduces non-linearity into the network. It maps the input to a node (neuron) to an output, allowing the network to learn complex relationships between inputs and outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d4a86d-1553-499a-9ed9-29e04b8c42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Common Activation Functions\n",
    "\n",
    "#Some common activation functions include:\n",
    "\n",
    "#1. Sigmoid\n",
    "#2. Rectified Linear Unit (ReLU)\n",
    "#3. Tanh (Hyperbolic Tangent)\n",
    "#4. Leaky ReLU\n",
    "#5. Softmax\n",
    "#6. Swish\n",
    "#7. Gelu (Gaussian Error Linear Units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f16a121-8a0d-4b83-a1ea-8d32ec5cf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Impact on Training and Performance\n",
    "\n",
    "#Activation functions affect the training process and performance by:\n",
    "\n",
    "#1. Introducing non-linearity, enabling learning of complex relationships.\n",
    "#2. Influencing the gradient flow during backpropagation.\n",
    "#3. Affecting the convergence rate and stability of training.\n",
    "#4. Impacting the network's ability to generalize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a6ecc0-5dae-4323-b5cd-f109dbf89f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Sigmoid Activation Function\n",
    "\n",
    "#The sigmoid function maps inputs to outputs between 0 and 1. Ïƒ(x) = 1 / (1 + exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce816204-59fa-4f8f-a193-c334c9a7d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Rectified Linear Unit (ReLU)\n",
    "\n",
    "#ReLU maps all negative inputs to 0 and all positive inputs to the same value.\n",
    "\n",
    "#f(x) = max(0, x)\n",
    "\n",
    "#ReLU differs from sigmoid:\n",
    "\n",
    "#1. Non-saturating for positive inputs\n",
    "#2. Computationally efficient\n",
    "#3. Less prone to vanishing gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9faa3be-f1a4-49d1-a807-d64ca4bafb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Benefits of ReLU over Sigmoid\n",
    "\n",
    "#Benefits of ReLU over sigmoid:\n",
    "#. Faster computation, Reduced risk of vanishing gradients, Improved convergence rate, Better handling of sparse data, Simplifies optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07cd1d1-0d75-41f8-a3a9-5bacabe72338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "#The hyperbolic tangent (tanh) activation function maps inputs to outputs between -1 and 1.\n",
    "\n",
    "#tanh(x) = 2 / (1 + exp(-2x)) - 1\n",
    "\n",
    "#or\n",
    "\n",
    "#tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "#Properties\n",
    "\n",
    "#1. S-shaped curve\n",
    "#2. Continuously differentiable\n",
    "#3. Output range: (-1, 1)\n",
    "#4. Symmetric around the origin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726079c-a6a4-4ea5-b782-fa1debdde627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
